{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbYlcBtYxRofH1ykwwxxKV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eunterko/MAT421/blob/main/ModuleD_Section_1_1_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chapter 1: Linear Algebra**"
      ],
      "metadata": {
        "id": "J_D77NUNUpAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *1.1 Introduction*"
      ],
      "metadata": {
        "id": "Z60TTQLUUrke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear algebra is mainly the study of mathematics through the use of vectors, matrices, and the various implications thereof. Because of this, linear algebra is an integral component of many data science or machine learning algorithms and methods. Many of the concepts introduced in linear algebra, both at a more basic and complex level, can be used to simplify the process of solving certain data science problems, as well as allowing us to tackle problems that we otherwise would be unable to solve. The following sections introduce the basics of linear algebra (section 1.2) and some more complicated applications (sections 1.3 and 1.4)."
      ],
      "metadata": {
        "id": "uSKXsKONUsDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *1.2 Elements of Linear Algebra*"
      ],
      "metadata": {
        "id": "IuUgTbLdUsns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start off with a few basic categorizations involving vectors. A linear combination is a new vector formed by multiplying a vector with a constant and adding up the results. Linear combinations lead to linear subspaces, a subset of a vector space. Similarly, spans are linear subspaces that are comprised of all linear combinations of a set of vectors, while column spaces are the spans of the columns of matrices."
      ],
      "metadata": {
        "id": "9Ko_MD42Usuy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectors are linearly independent if none of them are linear combinations of any of the other vectors. Linear independence is used to define the basis of a vector space: the basis of a matrix is a set of vectors that are linearly independent and span the matrix. Vector spaces can have multiple bases, however they all have the same number of elements, the same dimension. "
      ],
      "metadata": {
        "id": "flZGAvlB2Ep4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Orthogonality and normality are another important component of linear algebra. Two vectors are orthogonal if their dot product is zero, and a vector is normal if its dot product with itself is one. These two concepts are often expressed together as orthonormality, which is also an element of a vector basis. Let's look at a simple example, the standard basis."
      ],
      "metadata": {
        "id": "RZTA0SKn3jbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# standard basis\n",
        "v1 = np.array([1,0,0])\n",
        "v2 = np.array([0,1,0])\n",
        "v3 = np.array([0,0,1])\n",
        "\n",
        "dot_orth1 = np.dot(v1,v2)\n",
        "dot_orth2 = np.dot(v1,v3)\n",
        "dot_orth3 = np.dot(v2,v3)\n",
        "\n",
        "print(\"The dot product of v1 and v2 is\", dot_orth1)\n",
        "print(\"The dot product of v1 and v3 is\", dot_orth2)\n",
        "print(\"The dot product of v2 and v3 is\", dot_orth3)\n",
        "\n",
        "dot_norm1 = np.dot(v1,v1)\n",
        "dot_norm2 = np.dot(v2,v2)\n",
        "dot_norm3 = np.dot(v3,v3)\n",
        "\n",
        "print(\"\\nThe norm of v1 is\", dot_norm1)\n",
        "print(\"The norm of v2 is\", dot_norm2)\n",
        "print(\"The norm of v3 is\", dot_norm3)"
      ],
      "metadata": {
        "id": "7GWQjCBGUs7U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24cc60e8-af1b-4c57-eab8-1d81ffd97ca1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dot product of v1 and v2 is 0\n",
            "The dot product of v1 and v3 is 0\n",
            "The dot product of v2 and v3 is 0\n",
            "\n",
            "The norm of v1 is 1\n",
            "The norm of v2 is 1\n",
            "The norm of v3 is 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using orthogonality, we can formulate the Gram-Schmidt process, a means of generating a basis. Mainly, that given a set of linearly independent vectors, there exists an orthonormal basis set of vectors for the span of those linearly independent vectors."
      ],
      "metadata": {
        "id": "7U3Drgeo8rAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eigenvalues and eigenvectors make use of these prior methods to solve the equation Ax = Lx, where x is the nonzero eigenvector, A is a square matrix, and L is the eigenvalue. In general, if A is an m by m matrix, there exist at most m eigenvalues. Furthermore, the easiest way to find the eigenvalues of the matrix A is to diagonalize it: using the Gram-Schmidt process, use one eigenvector to form the orthonormal basis U, and then calculate U^T(A)U to find the eigenvalues along the diagonal."
      ],
      "metadata": {
        "id": "y8Jcw63z9Kju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *1.3 Linear Regression*"
      ],
      "metadata": {
        "id": "8M70UKulUt-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression is a method for solving models that depend linearly on their unknowns. Let's first look at QR decomposition, which solves the linear least squares problem, written as A = QR, where R is an upper triangular matrix. Python thankfully has the function qr(A) in scipy.linalg, which can take a numpy matrix and perform QR decomposition. Let's look at an example:"
      ],
      "metadata": {
        "id": "KMzf_BHjUuGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.linalg\n",
        "\n",
        "A = np.array([[1, 9, -7], [-6, 5, 2], [-8, -4, 3]])  \n",
        "Q,R = scipy.linalg.qr(A)\n",
        "\n",
        "print(\"A is \\n\", A)\n",
        "\n",
        "print(\"Q is \\n\", Q)\n",
        "\n",
        "print(\"R is \\n\", R)\n",
        "\n",
        "# checking A = QR\n",
        "print(\"QR is \\n\", np.dot(Q,R))"
      ],
      "metadata": {
        "id": "pCiDQRLLUuVJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32a35521-06f8-4b54-e8da-4ae404c6717c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A is \n",
            " [[ 1  9 -7]\n",
            " [-6  5  2]\n",
            " [-8 -4  3]]\n",
            "Q is \n",
            " [[-0.09950372 -0.80894303  0.57940503]\n",
            " [ 0.59702231 -0.51437246 -0.61561784]\n",
            " [ 0.79602975  0.28466147  0.53413901]]\n",
            "R is \n",
            " [[-10.04987562  -1.09454091   4.27865992]\n",
            " [  0.         -10.99099541   5.48784067]\n",
            " [  0.           0.          -3.68465386]]\n",
            "QR is \n",
            " [[ 1.  9. -7.]\n",
            " [-6.  5.  2.]\n",
            " [-8. -4.  3.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "More generally, with least squares problems, we are trying to solve Ax = b, where A is a matrix, and x and b are vectors. Often times, this involves rather approximating b with Ax, or vice versa. If A is a square matrix, we can more simply use the matrix inverse to solve this problem. As we saw above, QR decomposition can be used if A is a square matrix, but it can also be used is A is not square."
      ],
      "metadata": {
        "id": "YmTfVpIkB5Bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.linalg\n",
        "\n",
        "A = np.array([[1, 9], [-6, 5], [-8, -4]])  \n",
        "Q,R = scipy.linalg.qr(A)\n",
        "\n",
        "print(\"A is \\n\", A)\n",
        "\n",
        "print(\"Q is \\n\", Q)\n",
        "\n",
        "print(\"R is \\n\", R)\n",
        "\n",
        "# checking A = QR\n",
        "print(\"QR is \\n\", np.dot(Q,R))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6OYxu3pEZvL",
        "outputId": "c6fa8c19-790b-4ef6-825f-35f5acaaca43"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A is \n",
            " [[ 1  9]\n",
            " [-6  5]\n",
            " [-8 -4]]\n",
            "Q is \n",
            " [[-0.09950372 -0.80894303  0.57940503]\n",
            " [ 0.59702231 -0.51437246 -0.61561784]\n",
            " [ 0.79602975  0.28466147  0.53413901]]\n",
            "R is \n",
            " [[-10.04987562  -1.09454091]\n",
            " [  0.         -10.99099541]\n",
            " [  0.           0.        ]]\n",
            "QR is \n",
            " [[ 1.  9.]\n",
            " [-6.  5.]\n",
            " [-8. -4.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This now brings us to linear regression. Given an input data set, we find a linear function to fit the data. We rewrite this system as the minimization of the norm squared of a least squares problem, at which point we can use the methods discussed above to solve the problem, and find the linear fit."
      ],
      "metadata": {
        "id": "wxu9lZi_Eg-V"
      }
    }
  ]
}